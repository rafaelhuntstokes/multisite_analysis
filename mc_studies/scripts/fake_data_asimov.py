import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

"""
Script to test multisite likelihod signal extraction code using a fake dataset.

Multisite PDFs are defined as two gaussians, separated by some distance. Data is generated by sampling from these PDFs in a certain ratio Sig: Bkg.

The multisite and/or poisson likelihood is calculated for a grid search number of signal events.

The code is done in a modular way, ultimately to create logL curves of multisite for different PDF separations.
"""

def create_pdfs_and_dataset(separation, width, pdf_samples, sig_samples, backg_samples):
    """
    Create a single site and a multisite dlogL PDF as two gaussians of a given width and separation.
    Generate some samples and bin them to create the binned multisite PDFs used to evaluate the logL.

    Sample num_sig and num_backg respectively to create the dataset.
    """

    # create pdfs
    pdf_samples_sig    = np.random.normal(loc = separation, scale = width, size = pdf_samples) # offset the signal PDF from background (centred at 0)
    pdf_samples_backg  = np.random.normal(loc = 0, scale = width, size = pdf_samples)

    # generate dataset according to pdfs
    data_samples_sig   = np.random.normal(loc = separation, scale = width, size = sig_samples)
    data_samples_backg = np.random.normal(loc = 0, scale = width, size = backg_samples)

    # add the signal / background data together to make full dataset
    asimov_dataset = np.concatenate((data_samples_sig, data_samples_backg))

    # return the pdf and dataset samples
    return pdf_samples_sig, pdf_samples_backg, asimov_dataset

def evaluate_loglikelihood(true_num_signal, true_num_backg, asimov_dataset, pdf_sig_counts, pdf_backg_counts, pdf_binning):
    """
    Evaluate the multisite + poisson counting logL.
    """

    # find the PDF bin idx corresponding to each event in asimov dataset
    pdf_idx   = np.digitize(asimov_dataset, bins = pdf_binning) - 1 # -1 since digitize returns first bin with idx 1 but pdf idxed from 0

    # find the likelihood from signal and background PDFs for every event in asimov dataset
    prob_sig   = pdf_sig_counts[pdf_idx]
    prob_backg = pdf_backg_counts[pdf_idx] 

    # evalaute logL for each signal hypothesis (scanning across number of signal events --> using true value of backg events)
    multisite_vals = []
    poisson_vals   = []
    chi2_vals      = []
    for isig in range(int(true_num_signal*1.5)):

        total_hypothesised_counts = isig + true_num_backg
        fraction_sig_counts       = isig / total_hypothesised_counts
        
        # multisite loglikelihood
        multisite_l = fraction_sig_counts * prob_sig + (1-fraction_sig_counts) * prob_backg
        multisite_l = -np.sum(np.log(multisite_l))
        multisite_vals.append(multisite_l)

        # poisson counting loglikelihood
        poisson_l   = total_hypothesised_counts - len(asimov_dataset) * np.log(total_hypothesised_counts)
        poisson_vals.append(poisson_l)

        # chi2 result
        chi2        = (total_hypothesised_counts - len(asimov_dataset))**2 / total_hypothesised_counts
        chi2_vals.append(chi2)

    return multisite_vals, poisson_vals, chi2_vals

def create_output_plot(multsite, poisson, chi2, true_signal, pdf_sig, pdf_backg, pdf_binning):
    """
    Create a plot of the multisite, poisson, chi2 and full multisite + poisson loglikelihood as a function of signal hypothesis.
    """

    signal_hypothesis = np.arange(0, int(true_signal * 1.5), 1)
    
    # cast to array to perform elementwise shifts and additions
    chi2     = np.array(chi2)
    multsite = np.array(multsite)
    poisson  = np.array(poisson)
    full_l   = poisson + multsite # combien poisson and multisite likelihoods

    # shift curves so minima are at zero
    min_chi2_idx      = np.argmin(chi2)
    min_poisson_idx   = np.argmin(poisson)
    min_multisite_idx = np.argmin(multsite)
    min_full_l_idx    = np.argmin(full_l)

    diff_from_zero_chi2      = 0 - chi2[min_chi2_idx]
    diff_from_zero_multisite = 0 - multsite[min_multisite_idx]
    diff_from_zero_poisson   = 0 - poisson[min_poisson_idx] 
    diff_from_zero_full_l    = 0 - full_l[min_full_l_idx]

    # shift to put minima at zero
    chi2     = chi2 + diff_from_zero_chi2
    multsite = multsite + diff_from_zero_multisite
    poisson  = poisson + diff_from_zero_poisson
    full_l   = full_l + diff_from_zero_full_l

    # 3 plots: chi2 vs poisson likelihood; chi2 vs multisite likelihood; chi2 vs full likelihood
    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 8))
    
    # poisson
    axes[0].plot(signal_hypothesis, chi2, color = "blue", label = r"$\chi ^2$")
    ax2 = axes[0].twinx()
    ax2.plot(signal_hypothesis, poisson, color = "orange", label = "Poisson Likelihood")
    axes[0].set_xlabel("Number of Signal Events", fontsize = 20)
    axes[0].set_ylabel(r"$\chi ^2$", fontsize = 20)
    ax2.set_ylabel(r"$-log(\mathcal{L})$", fontsize = 20)
    axes[0].set_title(r"$\chi ^2$ vs Poisson LogLikelihood", fontsize = 20)
    axes[0].plot([], [], "", marker = "", color = "orange", label = "Poisson Likelihood")
    axes[0].axvline(true_signal, color = "red", label = "True Signal Number")
    axes[0].legend(fancybox = False, fontsize = 20)

    # multisite
    axes[1].plot(signal_hypothesis, chi2, color = "blue", label = r"$\chi ^2$")
    ax2 = axes[1].twinx()
    ax2.plot(signal_hypothesis, multsite, color = "orange", label = "Multisite Likelihood")
    axes[1].set_xlabel("Number of Signal Events", fontsize = 20)
    axes[1].set_ylabel(r"$\chi ^2$", fontsize = 20)
    ax2.set_ylabel(r"$-log(\mathcal{L})$", fontsize = 20)
    axes[1].set_title(r"$\chi ^2$ vs Multisite LogLikelihood", fontsize = 20)
    axes[1].plot([], [], "", marker = "", color = "orange", label = "Multisite Likelihood")
    axes[1].axvline(true_signal, color = "red", label = "True Signal Number")
    axes[1].legend(fancybox = False, fontsize = 20)

    # multisite + poisson
    axes[2].plot(signal_hypothesis, chi2, color = "blue", label = r"$\chi ^2$")
    ax2 = axes[2].twinx()
    ax2.plot(signal_hypothesis, full_l, color = "orange", label = "Full Likelihood")
    axes[2].set_xlabel("Number of Signal Events", fontsize = 20)
    axes[2].set_ylabel(r"$\chi ^2$", fontsize = 20)
    ax2.set_ylabel(r"$-log(\mathcal{L})$", fontsize = 20)
    axes[2].set_title(r"$\chi ^2$ vs Full LogLikelihood", fontsize = 20)
    axes[2].plot([], [], "", marker = "", color = "orange", label = "Full Likelihood")
    axes[2].axvline(true_signal, color = "red", label = "True Signal Number")
    axes[2].legend(fancybox = False, fontsize = 20)

    fig.tight_layout()

    plt.savefig("../plots/asimov_study/fake_dataset/test.png")
    plt.close()

    # plot the multisite PDFs
    plt.figure()
    plt.hist(pdf_sig, bins = pdf_binning, density = True, histtype = "step", label = "Signal")
    plt.hist(pdf_backg, bins = pdf_binning, density = True, histtype = "step", label = "Background")
    plt.title("Multisite PDFs")
    plt.xlabel(r"$\Delta log(\mathcal{L})$")
    plt.ylabel("Normalised Counts")
    plt.legend()
    plt.savefig("../plots/asimov_study/fake_dataset/pdfs.png")
    plt.close()

def run_analysis():
    """
    Function extracts the number of signal counts that maximises the loglikelihood for a set of 
    different signal/background PDF separation.
    """
    
    # define the number of signal and background counts in the dataset
    num_sig_events   = 3000
    num_backg_events = 7000

    # define the width and offset of the PDFs used to generate dataset and evaluate logL
    separation      = 5
    width           = 1
    num_pdf_samples = 20000                                          # how many events to sample from analytic PDF to create the binned PDF used to eval loglikelihood
    pdf_binning     = np.linspace(-separation*2, separation * 2, 50) # binning of PDF samples --> ensure range covers full spectrum of dataset otherwise gives out of range error
    pdf_sig, pdf_backg, asimov_dataset = create_pdfs_and_dataset(separation, width, num_pdf_samples, num_sig_events, num_backg_events)

    # bin the PDFs and return counts in each bin --> normalised so area = 1
    pdf_sig_counts,   _ = np.histogram(pdf_sig, bins = pdf_binning, density = True)
    pdf_backg_counts, _ = np.histogram(pdf_backg, bins = pdf_binning, density = True)

    # pad the pdf counts to account for any zero bins
    pdf_sig_counts[pdf_sig_counts == 0]     = 1e-6
    pdf_backg_counts[pdf_backg_counts == 0] = 1e-6

    # evaluate loglikelihood
    multisite_l, poisson_l, chi2_l = evaluate_loglikelihood(num_sig_events, num_backg_events, asimov_dataset, pdf_sig_counts, pdf_backg_counts, pdf_binning)

    # create the output plots
    create_output_plot(multisite_l, poisson_l, chi2_l, num_sig_events, pdf_sig, pdf_backg, pdf_binning)

run_analysis()
